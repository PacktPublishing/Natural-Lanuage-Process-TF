{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Learning Distributed Word Embeddings and Using Them for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this notebook, you'll learn to load texts into Tensorflow by converting words to numbers. \n",
    "You'll learn how to train distributed word representations, also known as word embeddings, by \n",
    "building your first tensorflow deep network for NLP. You'll compare these word embeddings to similar representations\n",
    "built with Latent Semantic Indexing (LSI). You'll learn how to save your embeddings for re-use, and how to load\n",
    "pre-trained embeddings which you borrow from the cloud. Finally, you'll learn how to use pre-trained embeddings\n",
    "for your first NLP task, categorizing documents. Along the way we'll point out many foundational techniques \n",
    "for NLP which will be helpful for you as your skills increase.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>A tip of the keyboard to the following sources that provided inspiration for this notebook:\n",
    "    <ul>\n",
    "        <li>1</li>\n",
    "    </ul>\n",
    "    </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "(Internal Links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import nltk\n",
    "import sklearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Magic Numbers\n",
    "To be used throughout our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabLength = 10000 #The number of unique words in our corpus which we'll use as inputs\n",
    "trainingEpochs = 1000 #How many training iterations we'll put our network through\n",
    "embeddingDim = 100 #How many dimensions in each embedding vector?\n",
    "skipgramWidth = 3 #How many words on either side of the target word should be included in its context?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Text Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: Make sure you've downloaded the NLTK text corpora following the directions at <a href=\"https://www.nltk.org/data.html\">https://www.nltk.org/data.html</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']']\n",
      "['ETYMOLOGY', '.']\n",
      "['(', 'Supplied', 'by', 'a', 'Late', 'Consumptive', 'Usher', 'to', 'a', 'Grammar', 'School', ')']\n",
      "['The', 'pale', 'Usher', '--', 'threadbare', 'in', 'coat', ',', 'heart', ',', 'body', ',', 'and', 'brain', ';', 'I', 'see', 'him', 'now', '.']\n"
     ]
    }
   ],
   "source": [
    "#Let's use the text of Melville's novel Moby Dick as our corpus. We'll load it from the NLTK corpus library.\n",
    "#Here's what the first couple sentences look like:\n",
    "i = 0\n",
    "for s in nltk.corpus.gutenberg.sents('melville-moby_dick.txt'):\n",
    "    i += 1\n",
    "    print(s)\n",
    "    if i > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 260819 word tokens in Moby Dick, of which 17152 are unique types.\n"
     ]
    }
   ],
   "source": [
    "#Let's count how many unique words are in Moby Dick.\n",
    "#We lower-case them first and remove punctuation. Here we're using the python string.lower() method and\n",
    "#a home-rolled punctuation stripper to do this normalization. In other NLP tasks you'll do additional\n",
    "#type of normalization including stripping non-ascii characters (pre-processing), stemming, and PoS tagging.\n",
    "from nltk import FreqDist\n",
    "from Chapter_03_utils import isPunctuation #Home-brewed function to test if token is punctuation\n",
    "\n",
    "mobyDickWords = FreqDist(w.lower() for w in nltk.corpus.gutenberg.words('melville-moby_dick.txt') if isPunctuation(w) == False)\n",
    "print(\"There are {} word tokens in Moby Dick, of which {} are unique types.\".format(len(nltk.corpus.gutenberg.words('melville-moby_dick.txt')),len(mobyDickWords)))\n",
    "#Phew! Melville was prolific!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert words to numbers\n",
    "<i>This is the first step in preparing the text data to be fed into a neural network or other machine learning model.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's inspect the terms and integer codes...\n",
      "{'the': 0, 'of': 1, 'and': 2, 'a': 3, 'to': 4, 'in': 5, 'that': 6, 'his': 7, 'it': 8, 'i': 9, 'he': 10}\n",
      "0: the\n",
      "1: of\n",
      "2: and\n"
     ]
    }
   ],
   "source": [
    "#Create dictionary of words and integer keys\n",
    "#We'll use a home-grown function even though there are several available, \n",
    "#including gensim.corpora.dictionary.Dictionary\n",
    "from Chapter_03_utils import terms2ints, ints2terms\n",
    "\n",
    "mobyDickTermsDict = terms2ints([term for (term, freq) in mobyDickWords.most_common(vocabLength)])\n",
    "print(\"Let's inspect the terms and integer codes...\")\n",
    "print({t:i for (t, i) in mobyDickTermsDict.items() if i < 11})\n",
    "mobyDickIntsDict = ints2terms(mobyDickTermsDict)\n",
    "#Test a few lookups in the reverse dictionary to make sure it's working\n",
    "for i in range(3):\n",
    "    print(\"{}: {}\".format(i, mobyDickIntsDict[i]))\n",
    "# Source: https://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/\n",
    "#https://adventuresinmachinelearning.com/word2vec-keras-tutorial/\n",
    "#    https://blog.cambridgespark.com/tutorial-build-your-own-embedding-and-use-it-in-a-neural-network-e9cde4a81296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded sentence: [400, 40, 1014]\n",
      "call\n",
      "invested\n"
     ]
    }
   ],
   "source": [
    "#Now we'll instantiate an encoder with our terms Dictionary and reverse dictionary\n",
    "# that can encode input lists as integers\n",
    "from Chapter_03_utils import IntEncoder\n",
    "enc = IntEncoder(mobyDickTermsDict, mobyDickIntsDict)\n",
    "#print(\"call: {}\".format(mobyDickIntsDict['call']))\n",
    "sentence = 'Call me Ishmael'\n",
    "result = enc.encode([word.lower() for word in nltk.word_tokenize(sentence) if isPunctuation(word) == False])\n",
    "print(\"Encoded sentence:\", result)\n",
    "print(mobyDickIntsDict[400])\n",
    "print(mobyDickIntsDict[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we'll use our encoder object to encode the words from Moby Dick as a sequence of integers\n",
    "words = [enc.lookupCode(w.lower()) for w in nltk.corpus.gutenberg.words('melville-moby_dick.txt') if isPunctuation(w) == False]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Deep Network for Word Embeddings\n",
    "<i>Thanks to <a href=\"https://adventuresinmachinelearning.com/word2vec-keras-tutorial/\">Adventures in Deep Learning's blog</a> for inspiring this section.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19, 625], [6037, 3], [10000, 13], [7325, 12], [71, 47], [3312, 536], [10000, 6783], [1063, 7803], [49, 10000], [10000, 2]] [1, 1, 1, 1, 0, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import make_sampling_table, skipgrams\n",
    "sampling_table = make_sampling_table(vocabLength + 1) #Add one to accommodate the out-of-vocab marker\n",
    "pairs, categories = skipgrams(words, vocabLength, window_size=skipgramWidth, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*pairs)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "print(pairs[:10], categories[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.lookupTerm(19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and build input layers using tf.keras functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Reshape, Dense\n",
    "# create some input variables\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocabLength, embeddingDim, input_length=1, name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = embedding(input_target)\n",
    "target = Reshape((embeddingDim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((embeddingDim, 1))(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a cosine similarity operation which will be output in a secondary model\n",
    "from tensorflow.keras.layers import dot\n",
    "similarity = dot([target, context], axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now perform the dot product operation to get a similarity measure\n",
    "dot_product = dot([target, context], axes=1)\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the primary training model\n",
    "from tensorflow.keras.models import Model\n",
    "model = Model(inputs=[input_target, input_context], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 100)       1000000     input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 100, 1)       0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 100, 1)       0           embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_5 (Dot)                     (None, 1, 1)         0           reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1)            0           dot_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            2           reshape_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,000,002\n",
      "Trainable params: 1,000,002\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network with Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine What the Network Has Learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Latent Semantic Index Word Representations from Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roshansantosh.wordpress.com Evaluating Term and Document Similarity Using Latent Semantic ANalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare LSI Word Representations to Deep Learning Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Trained Embeddings for Later Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus of Categorized Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Deep Network to Categorize Documents Using Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network, Test Accuracy on Hold-out Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

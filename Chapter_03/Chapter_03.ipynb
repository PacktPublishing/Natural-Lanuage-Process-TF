{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Learning Distributed Word Embeddings and Using Them for NLP\n",
    "<a id=\"Top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this notebook, you'll learn to load texts into Tensorflow by converting words to numbers. \n",
    "You'll learn how to train Latent Semantic Analysis (LSA) representations of words and documents using sklearn's TruncatedSVD method. You'll next train distributed word representations, also known as word embeddings, by \n",
    "building your first tensorflow neural network for NLP. You'll compare these word embeddings to similar representations\n",
    "built with Latent Semantic Indexing (LSI). You'll learn how to save your embeddings for re-use, and how to load\n",
    "pre-trained embeddings which you borrow from the cloud. Finally, you'll learn how to use pre-trained embeddings\n",
    "for your first NLP task, categorizing documents. Along the way we'll point out many foundational techniques \n",
    "for NLP which will be helpful for you as your skills increase.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [General Imports](#General-Imports)\n",
    "1. [Learn to Load, Explore, and Preprocess a Text Corpus](#Load-Explore)\n",
    "1. [Convert Words to Numbers](#Convert-Words-to-Numbers)\n",
    "1. [Train Latent Semantic Analysis Representations from Corpus](#LSA)\n",
    "1. [Train Skipgram Network for Word Embeddings](#Skipgrams)\n",
    "1. [Examine What the Skipgram Network Has Learned](#Examine-What-the-Skipgram-Network-Has-Learned)\n",
    "1. [Save Trained Embeddings for Later Use](#Save-Trained-Embeddings-for-Later-Use)\n",
    "1. [Re-load Pre-Trained Embeddings](#Re-load-Pre-Trained-Embeddings)\n",
    "1. [Putting It All Together: Your First NLP Task Using Embeddings and Deep Learning](#Putting-It-All-Together)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"General-Imports\"></a>\n",
    "## General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Load-Explore\"></a>\n",
    "## Learn How to Load, Explore, and Preprocess Text Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use some of our own functions to explore the text of Moby Dick, before we start applying deep learning to it.\n",
    "Tip: Make sure you've downloaded the NLTK text corpora following the directions at <a href=\"https://www.nltk.org/data.html\">https://www.nltk.org/data.html</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tour of Toolsets to Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use the text of Melville's novel Moby Dick as our corpus. We'll load it from the NLTK corpus library.\n",
    "#Here's what the first couple sentences look like:\n",
    "i = 0\n",
    "for s in nltk.corpus.gutenberg.sents('melville-moby_dick.txt'):\n",
    "    i += 1\n",
    "    print(s)\n",
    "    if i > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Melville's longest sentence.  Warning: it's long indeed!\n",
    "longestLen = 0\n",
    "longest = []\n",
    "for s in nltk.corpus.gutenberg.sents('melville-moby_dick.txt'):\n",
    "    thisLen = len(s)\n",
    "    if thisLen > longestLen:\n",
    "        longest = s\n",
    "        longestLen = thisLen\n",
    "print(longest)\n",
    "print(\"Longest sentence length = {}\".format(longestLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How about the longest word?\n",
    "wlen = 0\n",
    "longest = ''\n",
    "for w in nltk.corpus.gutenberg.words('melville-moby_dick.txt'):\n",
    "    if len(w) > wlen:\n",
    "        longest = w\n",
    "        wlen = len(w)\n",
    "print(longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's count how many unique words are in Moby Dick.\n",
    "#We lower-case them first and remove punctuation. Here we're using the python string.lower() method and\n",
    "#a home-rolled punctuation stripper to do this normalization. In other NLP tasks you'll do additional\n",
    "#type of normalization including stripping non-ascii characters (pre-processing), stemming, and PoS tagging.\n",
    "from nltk import FreqDist\n",
    "from Chapter_03_utils import isPunctuation #Home-brewed function to test if token is punctuation\n",
    "\n",
    "mobyDickWords = FreqDist(w.lower() for w in nltk.corpus.gutenberg.words('melville-moby_dick.txt') if ((isPunctuation(w) == False ) and (w not in nltk.corpus.stopwords.words('english'))))\n",
    "print(\"There are {} word tokens in Moby Dick, of which {} are unique types.\".format(len(nltk.corpus.gutenberg.words('melville-moby_dick.txt')),len(mobyDickWords)))\n",
    "print(\"Most common words in Moby Dick excluding stop words:\")\n",
    "print(\"-\" * 30)\n",
    "print(mobyDickWords.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use gensim to get same results\n",
    "texts = [[w.lower() for w in s if ((isPunctuation(w) == False ) and (w not in nltk.corpus.stopwords.words('english')))]  \n",
    "                                               for s in nltk.corpus.gutenberg.sents('melville-moby_dick.txt')]\n",
    "from collections import defaultdict #The nice thing about defaultdict objects is you don't have to initialize their values\n",
    "frequencies = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequencies[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take advantage of the heapq structure to choose the words with highest counts\n",
    "import heapq\n",
    "print(heapq.nlargest(10, frequencies.items(), key=lambda x : x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take advantage of gensim Dictionary object to make an efficient mapping between terms and integer ids\n",
    "gensim_dictionary = gensim.corpora.Dictionary(texts)                                                                    \n",
    "print(\"The dictionary has: \" +str(len(gensim_dictionary)) + \" tokens\")\n",
    "\n",
    "for (i, (k, v)) in enumerate(gensim_dictionary.token2id.items()):\n",
    "     if i < 10:\n",
    "        print(f'{k:{15}} {v:{10}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Peek into spaCy\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = nltk.corpus.gutenberg.raw('melville-moby_dick.txt')[:100000]\n",
    "doc = nlp(raw_text)\n",
    "\n",
    "# Analyze syntax: examine most frequent verbs\n",
    "verbFrequencies = defaultdict(int)\n",
    "for token in doc:\n",
    "    if token.pos_ == 'VERB':\n",
    "        verbFrequencies[token.lemma_] += 1\n",
    "print(heapq.nlargest(10, verbFrequencies.items(), key=lambda x : x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Convert-Words-to-Numbers\"></a>\n",
    "## Convert Words to Numbers\n",
    "<i>This is the first step in preparing the text data to be fed into a neural network or other machine learning model.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore bag-of-words vectors for the first few sentences of Moby Dick\n",
    "sentences = [s for (i, s) in enumerate(nltk.corpus.gutenberg.sents('melville-moby_dick.txt')) if i in [2, 3]]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tabulate the vocabulary of this mini-corpus so we can use it to create the vectorizer\n",
    "vocabulary_lists = [[w.lower() for w in s if isPunctuation(w) == False] for s in sentences]\n",
    "vocabulary = set([item for sublist in vocabulary_lists for item in sublist])\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize using bag-of-words representations for each document\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(binary=True, tokenizer=lambda doc: doc, preprocessor=None, vocabulary=vocabulary, lowercase=False)\n",
    "tdm = vectorizer.transform(vocabulary_lists)\n",
    "print(tdm.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(tdm.todense()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"LSA\"></a>\n",
    "## Train Latent Semantic Analysis Word Representations from Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking our inspiration from:\n",
    "<a href=\"https://roshansanthosh.wordpress.com/2016/02/18/evaluating-term-and-document-similarity-using-latent-semantic-analysis/\">Evaluating Term and Document Similarity Using Latent Semantic Analysis</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import some relevant packages\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make termXdocument matrix from top 10000 words\n",
    "vocab = [t for (t, f) in mobyDickWords.most_common(10000)]\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a sparse vectorizer using TFIDF weights and the vocab list we just created\n",
    "sparseVectorizer = TfidfVectorizer(vocabulary = vocab, tokenizer=lambda doc: doc, use_idf=True, lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start transforming Moby Dick from raw form into a corpus of documents we can feed into the TruncatedSVD\n",
    "# fetch a list of sentences\n",
    "sentences = [s for s in nltk.corpus.gutenberg.sents('melville-moby_dick.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess: lowercase and remove punctuation\n",
    "docs = [[w.lower() for w in s if isPunctuation(w) == False] for s in sentences]\n",
    "print(docs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the term X document matrix from the list of pre-processed sentences \n",
    "X = sparseVectorizer.fit_transform(docs)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import sklearn pipeline class and Normalizer function\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "svd = TruncatedSVD(n_components = 100, algorithm=\"arpack\")\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce the dimensionality of the sparse matrix using LSA\n",
    "lsa_X = lsa.fit_transform(X.T)\n",
    "print(X.shape)\n",
    "print(lsa_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClosestTerm(term,transformer,model):\n",
    " \n",
    "    index = transformer.vocabulary_[term]      \n",
    " \n",
    "    model = np.dot(model,model.T)\n",
    "    searchSpace =np.concatenate( (model[index][:index] , model[index][(index+1):]) )  \n",
    " \n",
    "    out = np.argmax(searchSpace)\n",
    " \n",
    "    if out<index:\n",
    "        return transformer.get_feature_names()[out]\n",
    "    else:\n",
    "        return transformer.get_feature_names()[(out+1)]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kClosestTerms(k,term,transformer,model):\n",
    " \n",
    "    index = transformer.vocabulary_[term]\n",
    " \n",
    "    model = np.dot(model,model.T)\n",
    " \n",
    "    closestTerms = {}\n",
    "    for i in range(len(model)):\n",
    "        if i != index:\n",
    "            closestTerms[transformer.get_feature_names()[i]] = model[index][i]\n",
    " \n",
    "    sortedList = sorted(closestTerms , key= lambda l : closestTerms[l], reverse=True)\n",
    "    \n",
    "    return(sortedList[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What has the LSA model learned about Ahab?\n",
    "(kClosestTerms(8, 'ahab', sparseVectorizer, lsa_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([d for d in docs if 'gritted' in d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What has the lsa model learned about whales?\n",
    "kClosestTerms(8, 'whale', sparseVectorizer, lsa_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([d for d in docs if 'cachalot' in d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Skipgrams\"></a>\n",
    "## Train Skipgram Network for Word Embeddings\n",
    "<i>Thanks to <a href=\"https://adventuresinmachinelearning.com/word2vec-keras-tutorial/\">Adventures in Deep Learning's blog</a> for inspiring this section.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specialized Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, dot\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from Chapter_03_utils import build_dataset\n",
    "from nltk import FreqDist, word_tokenize\n",
    "from nltk.corpus import gutenberg #Corpus readers for various literary texts\n",
    "from Chapter_03_utils import isPunctuation #Home-brewed function to test if token is punctuation\n",
    "from Chapter_03_utils import SimilarityCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and Magic Numbers\n",
    "To be used throughout the embeddings network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabSize = 2000 #How many words should we include as input?  Top n-most frequent\n",
    "windowSize = 3 #How many context words should be included on either side of the target word when choosing the skipgram pairs\n",
    "vectorDim = 300 #How many dimensions are included in the embedding space\n",
    "epochs = 1000 #Number of training epochs\n",
    "evalSetSize = 16     # Random set of words to evaluate similarity on.\n",
    "evalSetWindow = 100  # Only pick similarity evaluation samples from the head of the distribution (frequent terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build lexical resources for deep learning network\n",
    "def collect_data(corpusName, vocabulary_size=10000):\n",
    "    '''\n",
    "    learn and return a list of integer codes corresponding to the words of the text,\n",
    "    the term frequencies, and a regular and reverse dictionary of terms and integer codes.\n",
    "    Modified from https://adventuresinmachinelearning.com/word2vec-keras-tutorial/\n",
    "    '''\n",
    "    #Retrieve the requested text from the NLTK corpora collections\n",
    "    words = [w.lower() for w in nltk.corpus.gutenberg.words(corpusName) if isPunctuation(w) == False]\n",
    "    print(words[:7])\n",
    "    data, count, dictionary, reverse_dictionary = build_dataset(words,\n",
    "                                                                vocabulary_size)\n",
    "    del words  # Hint to reduce memory.\n",
    "    return data, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect data from corpus, encode, build lexical resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the raw text data into a list of integer codes, a frequency distribution of the terms, and a\n",
    "# dictionary and reverse_dictionary of terms:integer_codes\n",
    "data, count, dictionary, reverse_dictionary = collect_data('melville-moby_dick.txt', vocabulary_size=vocabSize)\n",
    "print(data[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose n number of indices from the top m most frequent terms to use to test similarity at different points in the training\n",
    "evalExamples = np.random.choice(evalSetWindow, evalSetSize, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample pairs of term-integer codes as skipgrams (target term, context term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now build the list of skipgrams to use for training, both positive and negative examples\n",
    "#Sampling frequencies for positive and negative skipgrams examples\n",
    "sampling_table = sequence.make_sampling_table(vocabSize)\n",
    "skipgramPairs, labels = skipgrams(data, vocabSize, window_size=windowSize, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*skipgramPairs)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "print(skipgramPairs[:10], labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine the distribution of 1s and 0s in the skipgram pairs\n",
    "from collections import Counter\n",
    "print(Counter(labels).keys())\n",
    "print(Counter(labels).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Substitute numbers found in the cell above\n",
    "print(reverse_dictionary[25])\n",
    "print(reverse_dictionary[883])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reverse_dictionary[141])\n",
    "print(reverse_dictionary[38])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reassure yourself that the labeling of the skipgrams is accurate\n",
    "from Chapter_03_utils import is_Sublist\n",
    "for s in nltk.corpus.gutenberg.sents('melville-moby_dick.txt'):\n",
    "    if is_Sublist(s, ['they', 'look'])==True:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and build input layers using tf.keras functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We're doing several tricky things here.  First, we're using the tf.keras functional api to define our model because we have two separate input laters, so the standard Sequential() api won't work for us. We instantiate each layer as a fucntion call whose argument is the list of layers feeding it.</p>\n",
    "<p>Secondly we define a separate model -- the similarityModel -- which we don't compile and train separately. We just hook it up to our SimilarityCallback function, so we can test word similarity at different points during the training to see how much our network has learned.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2 input layers with one input node each, for the skipgram target and context word codes\n",
    "input_target = Input((1,), name=\"input_target\")\n",
    "input_context = Input((1,), name=\"input_context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the embedding layer and two lookup layers for target and context\n",
    "embedding = Embedding(vocabSize, vectorDim, input_length=1, name='embedding')\n",
    "target = embedding(input_target)\n",
    "reshaped_target = Reshape((vectorDim, 1), name=\"reshaped_target_embedding\")(target)\n",
    "context = embedding(input_context)\n",
    "reshaped_context = Reshape((vectorDim, 1), name=\"reshaped_context_embedding\")(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cosine similarity operation which will be output in a secondary model\n",
    "similarity = dot([reshaped_target, reshaped_context], axes=1, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dot product layer for main model to get a similarity measure between target embedding and context embedding\n",
    "dot_product = dot([reshaped_target, reshaped_context], axes=1, normalize=False, name=\"dot_product\")\n",
    "reshaped_dot_product = Reshape((1,), name=\"reshaped_dot_product\")(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid', name=\"output\")(reshaped_dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the primary training model\n",
    "learningModel = Model(inputs=[input_target, input_context], outputs=output, name=\"learningModel\")\n",
    "learningModel.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a secondary validation model to run our similarity checks during training\n",
    "similarityModel = Model(inputs=[input_target, input_context], outputs=similarity, name=\"similarityModel\")\n",
    "sim_cb = SimilarityCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tip: Make sure you install pyplot and graphviz before attempting this step\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(learningModel, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Network with Skipgram Samples from Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, train the learning model, evaluating word similarity at specified epochs\n",
    "history = [] #Store loss for plotting\n",
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "       \n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "\n",
    "    loss = learningModel.train_on_batch([arr_1, arr_2], arr_3)\n",
    "  \n",
    "    if cnt % 100 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "        history.append((cnt, loss))\n",
    "    #Test what the model has learned at beginning and end of training    \n",
    "    if cnt in [0, (epochs - 1)]:\n",
    "        for testWord in ['ahab', 'whale', 'harpoon', 'boy', 'coffin']:\n",
    "            sim_cb.probe_word(testWord, dictionary, reverse_dictionary, vocabSize, similarityModel)\n",
    "        sim_cb.run_sim(evalSetSize, evalExamples, reverse_dictionary, vocabSize, similarityModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Examine-What-the-Skipgram-Network-Has-Learned\"></a>\n",
    "## Examine What the Skipgram Network Has Learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "N = 10 #Size of moving average window\n",
    "y_losses = [l for (e,l) in history]\n",
    "x_epochs = [e for (e,l) in history]\n",
    "history_step_size = 100\n",
    "ma_epochs = x_epochs[:(epochs - (N * history_step_size))]\n",
    "loss_line = plt.plot(x_epochs, y_losses)\n",
    "#Also plot the moving average, to show the trend more clearly\n",
    "mae_line = plt.plot(mae, np.convolve(y_losses, np.ones((N,))/N, mode='valid'))\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(labels = ['Loss', 'Moving Average Loss'])\n",
    "#plt.show()\n",
    "plt.savefig('loss_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Save-Trained-Embeddings-for-Later-Use\"></a>\n",
    "## Save Trained Embeddings for Later Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer.get_weights() returns a list of numpy ndarrays containing the weights\n",
    "embedding_weights = embedding.get_weights()[0]\n",
    "print(type(embedding_weights))\n",
    "print(embedding_weights.shape)\n",
    "print(\"First value in weights matrix = {}.\".format(embedding_weights[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the weights array to file\n",
    "filename = 'moby_weights.csv'\n",
    "np.savetxt(filename, embedding_weights, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's examine the size of the file to see what it contains\n",
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "size = os.path.getsize(filename)\n",
    "print(\"Weights file is {} bytes.\".format(size))\n",
    "rows = file_len(filename)\n",
    "print(\"Weights file has {} rows.\".format(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Re-load-Pre-Trained-Embeddings\"></a>\n",
    "## Re-load Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, 'rt') as fh:\n",
    "    new_weights = np.loadtxt(fh, delimiter=\",\")\n",
    "print(new_weights.shape)\n",
    "print(new_weights[0][0])\n",
    "embedding.set_weights([new_weights]) #set_weights() expects a list of ndarrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Putting-It-All-Together\"></a>\n",
    "## Putting It All Together: Your First NLP Task Using Deep Learning and Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Corpus of Categorized Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use the pre-labeled newsgroups data provided by sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['talk.religion.misc','comp.graphics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download some labeled newsgroup postings\n",
    "dataset_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "dataset_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                             shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The dataset is an objbect which contains a data member (list of strings) \n",
    "#and a target member (list of integer codes)\n",
    "print(Counter(dataset_train.target).keys())\n",
    "print(Counter(dataset_train.target).values())\n",
    "print(dataset_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a binary labels list of reference category and other\n",
    "ref = dataset_train.target_names.index('talk.religion.misc')\n",
    "\n",
    "def code_ref(value):\n",
    "    if value == ref:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "train_y = list(map(code_ref, dataset_train.target))\n",
    "test_y = list(map(code_ref, dataset_test.target))\n",
    "train_y = np.asarray(train_y)\n",
    "test_y = np.asarray(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's an example of what the data look like...\n",
    "print(dataset_train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-Trained Embeddings Using Gensim and Homegrown Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This step takes a few minutes to load...\n",
    "word_vectors = gensim.models.KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin.gz', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2ints = {w:(i+1) for (i, w) in enumerate(word_vectors.wv.vocab) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints2words = {(i+i):w for (i,w) in enumerate(word_vectors.wv.vocab)} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the keyed vectors into an array of weights\n",
    "<i>This model is a dumbed-down version of the one found <a href=\"https://keras.io/examples/pretrained_word_embeddings/\">here in the keras.io docs.</a></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NUM_WORDS = 2000\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "RAW_FEATURES = 2000 #Total number of raw words to include in tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get vocab list of words in the posts, train and test set\n",
    "tokens = []\n",
    "for post in dataset_train.data:\n",
    "    for word in word_tokenize(post):\n",
    "        tok = word.lower()\n",
    "        if isPunctuation(tok) == False:\n",
    "            tokens.append(tok)\n",
    "for post in dataset_test.data:\n",
    "    for word in word_tokenize(post):\n",
    "        tok = word.lower()\n",
    "        if isPunctuation(tok) == False:\n",
    "            tokens.append(tok)\n",
    "            \n",
    "postsVocab = FreqDist(tokens)        \n",
    "postsTerms = [w for (w, i) in postsVocab.most_common(RAW_FEATURES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(postsVocab))\n",
    "embedding_matrix = np.zeros(((num_words + 1), EMBEDDING_DIM))\n",
    "i = 0\n",
    "for (w, f) in postsVocab.most_common(MAX_NUM_WORDS):\n",
    "    if w in words2ints.keys():\n",
    "        embedding_vector = word_vectors.wv.get_vector(w)\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Chapter_03_utils import IntEncoder, terms2ints, ints2terms\n",
    "postsTermsDict = terms2ints(postsTerms)\n",
    "postsReverseTermsDict = ints2terms(postsTermsDict)\n",
    "enc = IntEncoder(postsTermsDict, postsReverseTermsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train = []\n",
    "for post in dataset_train.data:\n",
    "    tokens = [enc.lookupCode(t) for t in [tok.lower() for tok in word_tokenize(post) if isPunctuation(tok) == False]]\n",
    "    sequences_train.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = []\n",
    "for post in dataset_test.data:\n",
    "    tokens = [enc.lookupCode(t) for t in [tok.lower() for tok in word_tokenize(post) if isPunctuation(tok) == False]]\n",
    "    sequences_test.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "train_X = pad_sequences(sequences_train, maxlen = MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "test_X = pad_sequences(sequences_test, maxlen = MAX_SEQUENCE_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll again use the keras functional api to build a network, this one to\n",
    "# categorize the postings\n",
    "catModel = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make instance of embeddings layer \n",
    "from tensorflow.keras.initializers import Constant\n",
    "embedding_layer = Embedding(num_words + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "catModel.add(sequence_input)\n",
    "catModel.add(embedding_layer)\n",
    "mean = Lambda(lambda x: tf.keras.backend.mean(x, axis=1))\n",
    "catModel.add(mean)\n",
    "h1 = Dense(128, activation='relu')\n",
    "catModel.add(h1)\n",
    "h2 = Dense(16, activation='relu')\n",
    "catModel.add(h2)\n",
    "output = Dense(1, activation='sigmoid')\n",
    "catModel.add(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catModel.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = catModel.fit(train_X, train_y,\n",
    "          batch_size=128,\n",
    "          epochs=50,\n",
    "            validation_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = catModel.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = zip(prediction, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for (yhat, y) in results:\n",
    "    i += 1\n",
    "    print(\"{} | {}\".format(yhat, y))\n",
    "    if i > 20:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot and examine the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot and discuss the confusion matrix.\n",
    "# Peace with honor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
